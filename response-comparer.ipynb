{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d0b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q litellm\n",
    "%pip install -q matplotlib numpy pandas httpx tqdm jupyter ipywidgets aiofiles\n",
    "%pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdfa8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion, batch_completion\n",
    "\n",
    "from httpx import HTTPStatusError\n",
    "from litellm.exceptions import NotFoundError\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dafe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a8b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0.01\n",
    "\n",
    "def comparer_prompt(responses: List[str]) -> (str, str):\n",
    "    system_output = \"\"\"\n",
    "    You are an assistant that takes in the multiple responses from the user,\n",
    "    and then writes unique pros and cons for each of the models' responses relative to other responses.\n",
    "\n",
    "    ---\n",
    "    \n",
    "    Here's a basic example below. Please use this output structure:\n",
    "\n",
    "    User input:\n",
    "    [response 1]\n",
    "    'hi'\n",
    "\n",
    "    [response 2]\n",
    "    'hello there'\n",
    "\n",
    "    Your output:\n",
    "    [response 1]\n",
    "    Pros:\n",
    "    1. shorter\n",
    "    Cons:\n",
    "    2. cold\n",
    "\n",
    "    [response 2]\n",
    "    Pros:\n",
    "    1. more formal\n",
    "    Cons:\n",
    "    2. longer\n",
    "\n",
    "    ---\n",
    "    \"\"\"\n",
    "    user_output = \"\"\n",
    "    for i in range(len(responses)):\n",
    "        user_output += f\"[response {i + 1}]\\n\\'\" + responses[i] + \"\\'\\n\\n\"\n",
    "\n",
    "    return system_output, user_output\n",
    "\n",
    "def judger_prompt(responses: List[str], pros_and_cons: str) -> (str, str):\n",
    "    system_output = \"\"\"\n",
    "    You are an assistant that takes in the multiple responses and the differences (pros and cons) from the user.\n",
    "    Using this, come up with the best response possible combining these responses. \n",
    "\n",
    "    ---\n",
    "    \n",
    "    Here's a basic example below. Please use this output structure:\n",
    "\n",
    "    User input:\n",
    "    [response 1]\n",
    "    'hi'\n",
    "\n",
    "    [response 2]\n",
    "    'hello there'\n",
    "\n",
    "    [pros and cons]\n",
    "    [response 1]\n",
    "    Pros:\n",
    "    1. shorter\n",
    "    Cons:\n",
    "    2. cold\n",
    "\n",
    "    [response 2]\n",
    "    Pros:\n",
    "    1. more formal\n",
    "    Cons:\n",
    "    2. longer\n",
    "\n",
    "    Your output:\n",
    "    Hi there\n",
    "\n",
    "    ---\n",
    "    \"\"\"\n",
    "    user_output = \"\"\n",
    "    for i in range(len(responses)):\n",
    "        user_output += f\"[response {i + 1}]\\n\\'\" + responses[i] + \"\\'\\n\\n\"\n",
    "    user_output += f\"[pros and cons]\\n\" + pros_and_cons\n",
    "    \n",
    "    return system_output, user_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78c4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_responses = []\n",
    "\n",
    "comparer_system, comparer_user = comparer_prompt(list_of_responses)\n",
    "response = completion(\n",
    "          model=\"gpt-4.1-2025-04-14\",\n",
    "          messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": comparer_system,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": comparer_user,\n",
    "            },\n",
    "          ],\n",
    "          temperature=temp,\n",
    "    )\n",
    "comparer_response = str(response.choices[0].message.content)\n",
    "judger_system, judger_user = judger_prompt(list_of_responses, comparer_response)\n",
    "response = completion(\n",
    "          model=\"gpt-4.1-2025-04-14\",\n",
    "          messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": judger_system,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": judger_user,\n",
    "            },\n",
    "          ],\n",
    "          temperature=temp,\n",
    "    )\n",
    "print(str(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edf139a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
